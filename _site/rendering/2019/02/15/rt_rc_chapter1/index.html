<!DOCTYPE html>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta charset="utf-8">
<meta name="author" content="Darryl Gouder">
<meta name="description" content="Blog about code and stuff...">
<title> Ray Tracing The Rest of Your Life: A reader's companion, Chapter 1 › Darryl's Pixels</title>
<link rel="canonical" href="http://localhost:4000/rendering/2019/02/15/rt_rc_chapter1/">
<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,300italic,400italic" rel="stylesheet">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link href="/basic.css" rel="stylesheet">
<link href="/highlight.css" rel="stylesheet">
<link href="/index.css" rel="stylesheet">
<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Darryl&apos;s Pixels" />
<script src="//darrylgouder.disqus.com/embed.js" async></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<header>
  <h1><a href="">Darryl's Pixels</a></h1>
  <nav>
    <div><a href="/">Home</a><a href="/archive/">Archive</a><a href="/about/">About</a><a href="/solstice/">Solstice</a></div>
    <div><a href="https://twitter.com/dargouder"><i class="fa fa-twitter"></i></a><a href="https://github.com/dargouder"><i class="fa fa-github"></i></a><a href="mailto:dargouder@gmail.com"><i class="fa fa-envelope"></i></a></div>
  </nav>
</header>
<script src="http://localhost:4000/assets/image-comparer.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<main>
  <article>
    <header>
      <h1><a href="/rendering/2019/02/15/rt_rc_chapter1/">Ray Tracing The Rest of Your Life: A reader's companion, Chapter 1</a></h1>
      <time datetime="2019-02-15T00:00:00+01:00">February 15, 2019</time>
    </header>
<p>This blog series is a set of notes that expand on Dr Shirley’s “Ray Tracing The Rest of Your Life” book. The book is compact, introducing all the tools that one will need in their arsenal to build a sophisticated ray tracer. I found it very refreshing in terms of simplifying otherwise esoteric concepts and bridging the gap between simple mathematical concepts and their application in ray tracing.</p>

<p>My journey through the book raised many questions, which Dr Shirley was very helpful in answering and explaining. Along the way I read many different articles and saw different videos to understand things. I am probably not the only person who needed help understanding things and won’t be the last, so I thought it would be a good idea to share the information that I gathered.</p>

<p>The format of this post is unconventional  - you should have a copy of <a href="http://www.realtimerendering.com/raytracing/Ray%20Tracing_%20the%20Rest%20of%20Your%20Life.pdf" target="_blank">the book</a> (ideally the PDF as it is the most recent updated) open in another tab. I’ll try to guide you as best as I can throughout the book by asking you to look at the book and come back when you reach a certain point.</p>
<p align="center" style="color:red;">
<b>These statements will be in bold and red! </b></p>

<p>Re-reading is also very important - it’s very hard to understand everything at a first read. Don’t worry too much if you don’t get everything immediately. These are not straightforward concepts so just keep at it. If you feel truly stuck, reach out to me and we’ll try to clear things up. With all that said, let’s start.</p>

<h1 id="chapter-1-a-simple-monte-carlo-program">Chapter 1: A Simple Monte Carlo Program</h1>

<p>The first chapter is very straightforward. Demonstrating Monte Carlo by estimating \(\pi\) is a prevalent computational statistics example.</p>

<p>The code snippet for this that is presented in the book is very simple and self explanatory. A point is generated inside a square and if it is inside the circle, we increase the counter tracking the number of points inside the circle. We divide this final count by the total number of points we generate to get an average answer to \(\pi\).</p>

<p align="center" style="color:red;">
<b>
It’s time to go look at the book - read up until the you get to the 2nd code snippet, involving the running \(\pi\) calculation program.
</b>
</p>

<p>One of the questions of those uninitiated in probability and Monte Carlo theory might be: How does this type of computation work and how does it end up giving us an estimate of \(\pi\)? This stems from what is known as the <strong>expected value</strong>.</p>

<p>A classic example in explaining the expected value is trying to answer the question:</p>

<p align="center">
What is the probability that I will get a heads, when I flip a fair coin?
</p>

<p>We know intuitively that it is 0.5. However what the expected value theory shows us is that if we</p>

<ol>
  <li>get an actual fair coin,</li>
  <li>start flipping it,</li>
  <li>take note of each result,</li>
  <li>count the number of heads and tails,</li>
  <li>average them,</li>
</ol>

<p>We will probably get a result close to 0.5. The more times we toss the coin, the closer to 0.5 the answer will be.</p>

<p>The mathematical notation used to describe the expected value is:</p>

\[E[X] = \sum_{i=1}^k x_i p_i = x_1 p_1 + x_2 p_2 + ... + x_k p_k\]

<ul>
  <li>\(E[X]\) is the expected value of \(X\), our statistical experiment.</li>
  <li>\(x_i\) is the result of each sample that we compute of our function</li>
  <li>\(p_i\) is the probability weight of this sample. They must sum up to 1.</li>
</ul>

<p>I’ll use another example rather than a coin toss to tie with the above equation. If we had a fair 6-sided dice, the expected value would be:</p>

\[E[X] = \sum_{i=1}^k x_i p_i = \frac{1}{6} + \frac{2}{6} + \frac{3}{6} +  \frac{4}{6} + \frac{5}{6} + \frac{6}{6} = 3.5\]

<p>where \(x_i\) was the dice roll and $$ \frac{1}{6} was the probability weight. If dice wasn’t fair, the weights would be different - that doesn’t matter as long as they sum up to one.</p>

<p>Back to our original monte carlo example, the reason why this works is a theorem known as the <strong>Law of Large Numbers</strong>. The weak law of large numbers states that if we take a number of samples and average them, it will <em>PROBABLY</em> converge to the expected value. The strong law of large numbers states that it will converge <em>ALMOST SURELY</em> (with probability 1). We’re not going to go down this rabbit hole, the one of interest to us in the weak law.</p>

<p>An important term that we should define is <strong>variance</strong>. In our example, we were trying to compute \(\pi\), which makes this  the mean we’re trying to estimate. The variance, in very informal language, measures how spread out our sample computations are from this mean. The lower the variance, the less spread out the samples are and the more accurate our computed mean is to the expected value (the expected value is also known as the mean). If we have high variance, this means our samples are spread out, and we might be using an ineffective function to computationally find our mean. There are ways to quantify variance which are useful for estimating the error of our Monte Carlo techniques, however I think it would not be fruitful for now to discuss those.</p>

<p>The <strong>Law of Diminishing Returns</strong> explains the idea that the more samples we take, the more we need than before, to get an accurate answer.  In terms of numbers, if you want to half the error that you’re getting, you’ll need to quadruple the amount of samples. Here is how the error between our estimated value of \(\pi\) and actual \(\pi\) decreases as we compute more samples.</p>

<p align="center">
<img src="http://localhost:4000/assets/posts/rt_rc_chapter1/pi_estimate.png" alt=" $$ \pi $$ Estimate error" />
</p>

<p>Notice how after sample 200, the error doesn’t decrease that much. This doesn’t hold a lot of significance for now but it’s good to keep this in mind.</p>

<p align="center" style="color:red;">
<b>Now would be a good time to go read the rest of the chapter.</b>
</p>

<p>Stratification is a way of intelligently placing your samples to estimate your function better. If you look at my <a href="https://dargouder.github.io/darryls-pixels/mathematics/2018/07/04/prngs/" target="_blank">blog post</a> about random numbers, I show that a good PRNG (Pseudo-Random Number Generator) should generate uniformly distributed points. When you’re dealing with multiple dimensions a PRNG’s inherent randomness isn’t enough. Think of dimensions this way: you need 2 uniform random numbers for your primary ray, then 2 for your lens, then another 2 for your next bounce and so forth. Everytime, you’re adding another dimension and although your random numbers are unifornm if we look at them at being in the same dimension, it may not necessarily be the case when used in these different dimensions. To ensure true uniformity, without having clustered samples, we stratify the samples we take.</p>

<p>The careful selection of samples to minimize variance is an ongoing area of research, that is an absolutely fascinating subject and one of my favourite topics in CG. Dr. Shirley expands on this topic is in this blog post: <a href="http://psgraphics.blogspot.com/2018/10/flavors-of-sampling-in-ray-tracing.html" target="_blank">Flavors of sampling in ray tracing</a>. Leonhard Grünschloß has some excellent implementations of different <a href="http://gruenschloss.org/" target="_blank">Quasi-Monte Carlo based samplers</a>.</p>

<p>I’ll slightly re-visit stratification at the end of Chapter 2, once we define the Monte Carlo Integral and importance sampling. The topic warrants tons of blog posts, maybe someday…</p>

<p>The next post will focus on Chapter 2, where I’ll delve ever so slightly into Monte Carlo theory and we’ll perform some interesting computations.</p>

<p>If you have any questions and feedback, feel free to comment or contact me via twitter/email.</p>

    
    
    <hr><div id="disqus_thread"></div>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
  </article>
</main>


<footer>
    <a href="/rendering/2019/02/21/rt_rc_chapter2/">« Ray Tracing The Rest of Your Life: A reader's companion, Chapter 2</a>
    <a href="/mathematics/2018/07/04/prngs/">Pseudo Random Number Generation »</a>
</footer>

