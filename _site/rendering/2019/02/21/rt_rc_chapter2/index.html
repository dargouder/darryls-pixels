<!DOCTYPE html>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta charset="utf-8">
<meta name="author" content="Darryl Gouder">
<meta name="description" content="Blog about code and stuff...">
<title> Ray Tracing The Rest of Your Life: A reader's companion, Chapter 2 › Darryl's Pixels</title>
<link rel="canonical" href="http://localhost:4000/rendering/2019/02/21/rt_rc_chapter2/">
<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,300italic,400italic" rel="stylesheet">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link href="/basic.css" rel="stylesheet">
<link href="/highlight.css" rel="stylesheet">
<link href="/index.css" rel="stylesheet">
<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Darryl's Pixels" />
<script src="//darrylgouder.disqus.com/embed.js" async></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<header>
  <h1><a href="">Darryl's Pixels</a></h1>
  <nav>
    <div><a href="/">Home</a><a href="/archive/">Archive</a><a href="/about/">About</a><a href="/solstice/">Solstice</a></div>
    <div><a href="https://twitter.com/dargouder"><i class="fa fa-twitter"></i></a><a href="https://github.com/dargouder"><i class="fa fa-github"></i></a><a href="mailto:dargouder@gmail.com"><i class="fa fa-envelope"></i></a></div>
  </nav>
</header>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<main>
  <article>
    <header>
      <h1><a href="/rendering/2019/02/21/rt_rc_chapter2/">Ray Tracing The Rest of Your Life: A reader's companion, Chapter 2</a></h1>
      <time datetime="2019-02-21T00:00:00+00:00">February 21, 2019</time>
    </header>
<p>Welcome back! If you’re joining from the 1st post, you can skip ahead and start reading from the Chapter 2 title. If not, here’s a <a href="https://dargouder.github.io/darryls-pixels/rendering/2019/02/15/rt_rc_chapter1/" target="_blank">link</a> to the first post as there’s some information in the beginning about this blog series.</p>

<h1 id="chapter-2-one-dimensional-mc-integration">Chapter 2: One Dimensional MC Integration</h1>

<p>Monte Carlo is widely used to solve integrals of higher dimensions, where an analytical method is not possible, or other integral computational methods are intractable. Dr. Shirley selects a simple integral to show how effective Monte Carlo integration is.</p>

<p align="center" style="color:red;">
<b>
Start reading the chapter right until you get to the code that computes the integral. Don’t worry if you don’t get it, I’m going to explain everything.
</b>
</p>

<p>The integral is <script type="math/tex">\int_0^2 x^2 dx</script> which will compute the area under the curve, with the limits 0 to 2. So how do we translate this into a Monte Carlo estimate? I was a bit confused by the sudden appearance of the 2 being multiplied with the average(x^2, 0, 2) function. Why are we multiplying the sample average by 2? Well as it turns out, this how we perform Monte Carlo integration. A generic integral of the form:</p>

<script type="math/tex; mode=display">F = \int_a^b f(x)</script>

<p>can be estimated with a Monte Carlo estimator by performing</p>

<script type="math/tex; mode=display">(b-a) \frac{1}{N} \sum_{i=0}^{N-1} f(X_i)</script>

<p><script type="math/tex">x_i</script> are uniformly distributed random numbers between 0 and 1, and <script type="math/tex">N</script> is the number of samples you pick.</p>

<p align="center" style="color:red;">
<b>
Read the book right up until he proposes making his own PDF. I’ll lengthen the explanation and drive in a bit of probability, enough to show you the ropes but not enough to bore you (hopefully).
</b>
</p>

<p>So the area function Dr. Shirley is trying to calculate is:</p>

<script type="math/tex; mode=display">\int_0^2 C' r dr = \frac{1}{2} [C'r^2]_0^2 = 2C'
\\
1 = 2C'
\\
C' = \frac{1}{2}</script>

<p>Knowing that <script type="math/tex">p(r) = C'r</script> this leads to <script type="math/tex">p(r) = \frac{r}{2}</script>. The rest is pretty readable and understandable so I’ll move onto something else.</p>

<p>A short introduction into probability theory with a bit more formality will go a long way into helping you understand what Dr. Shirley computes in the book and other future graphics problems.</p>

<p>We’ve been talking about random variables for a while. We know that generating a uniform random variable means that we use a function to generate a number in a particular range and since we stipulated that it is uniform, this also means that any other number in that range could have been picked with the exact same probability.</p>

<p>We’ll introduce some terms.</p>
<ul>
  <li><script type="math/tex">X</script> represents the random variable. Since we’re using random variables to estimate something, this also is referred to as a random variable!</li>
  <li><script type="math/tex">P(X)</script> represents the probability of <script type="math/tex">X</script>.</li>
  <li><script type="math/tex">P(X = x)</script> represents the probability that <script type="math/tex">X</script> is a particular value <script type="math/tex">x</script>.</li>
</ul>

<p>To exemplify these terms, in our statistical experiment we are going to toss a coin twice.</p>
<ul>
  <li><script type="math/tex">X</script> is the number of heads that we get.  This is the random variable that is the result of our 2 coin tosses. <script type="math/tex">X</script> will either be 0 (<script type="math/tex">TT</script>), 1(<script type="math/tex">HT</script> or <script type="math/tex">TH</script>) or 2(<script type="math/tex">HH</script>).</li>
  <li><script type="math/tex">P(X)</script> will be the probability of getting 0, 1 or 2 heads.</li>
  <li><script type="math/tex">P(X = x)</script> will be a particular sequence of coin tosses, such as <script type="math/tex">TT</script> or <script type="math/tex">TH</script>.</li>
</ul>

<p>The cumulative distribution function (CDF) and the probability mass/density function (pmf/PDF) are the 2 pillars of understanding the nature of a random variable. The reason I used mass/density is because mass is used when we are describing a discrete distribution, and density is used in the continuous case. In this example, it’s the pmf (a coin toss is a discrete experiment) that I’ll be talking about, however the same ideas hold for the PDF.</p>

<p>This is the CDF of the experiment above:</p>

<table>
  <thead>
    <tr>
      <th><script type="math/tex">P(X)</script></th>
      <th>CDF <script type="math/tex">% <![CDATA[
P(X <=x) %]]></script></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.25</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.75</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>As you may have noticed, the CDF is maxed out at one. 1 is the upper limit of the probability that an event from an experiment occurs - 0 is the lower limit. It is called cumulative because the probability distribution accumulates with each event. If you haven’t understood what that table is representing, let’s use our words!</p>

<ul>
  <li><script type="math/tex">P(X=0)</script> means that that probability of getting no heads is 0.25.</li>
  <li><script type="math/tex">% <![CDATA[
P(X<=1) %]]></script> is <script type="math/tex"></script>P(X=1) + P(X=0)<script type="math/tex"></script>. <script type="math/tex">P(X=1)</script> is <script type="math/tex">TH</script> and <script type="math/tex">HT</script> and therefore we’re adding 0.5 to the probability of getting <script type="math/tex">TT</script> which is 0.25, hence 0.75.</li>
  <li><script type="math/tex">% <![CDATA[
P(X<=2) %]]></script> is just the probability of <script type="math/tex">HH</script> (0.25) + the previous 0.75, our final result being 1.</li>
</ul>

<p>This is what the CDF looks like when we plot it</p>

<p align="center">
<img src="http://localhost:4000/assets/posts/rt_rc_chapter2/coin_toss_cdf.png" alt="CDF of 2 coin tosses " />
</p>

<p>So can you guess what the PMF will look like? Hint: the pmf will describe the probability of each occurring coin toss .</p>

<table>
  <thead>
    <tr>
      <th><script type="math/tex">P(X)</script></th>
      <th>PMF <script type="math/tex">P(X =x)</script></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0 heads</td>
      <td>0.25</td>
    </tr>
    <tr>
      <td>1 heads</td>
      <td>0.5</td>
    </tr>
    <tr>
      <td>2 heads</td>
      <td>0.25</td>
    </tr>
  </tbody>
</table>

<p>If you sum up all the probabilities, they’ll add up to 1. So the PMF is showing us the probability of an event occurrence and CDF is showing us the probability of a range of events occuring. I know I keep on repeating this statement, but it’s common to not fully grasp the difference between the CDF and PMF and be unsure of their purpose. If you look hard at both, see you can go from the CDF to the PMF and vice versa.</p>

<p>So what happens if we’re dealing with continuous cases?
Well for starters, the PMF changes to a PDF. We are also usually presented with functions to represent the CDF and PDF. To find the CDF from the PDF, we integrate the PDF from <script type="math/tex">-\infty</script> to <script type="math/tex">x</script>, where <script type="math/tex">x</script> is a placeholder variable.</p>

<p>I’m not going to show an example because at this point I think you should go back to the book but wait until this final concept.</p>

<p>We’ve mentioned that we have been generating uniform random variables using a PRNG. In many cases however, we would like to take these uniform random numbers and would like to make them follow some other kind of distribution. We want to <em>transform</em> them. The way to go about this is a technique known as the Inverse Transform method. The technique is very simple and very powerful:
 Let’s say we have a random variable <script type="math/tex">X</script> , that has a CDF <script type="math/tex">F(X)</script>. We want to generate values distributed according to this distribution. The steps to do this are:</p>

<ol>
  <li>Generate a uniform random number u between <script type="math/tex">[0,1]</script>.</li>
  <li>Find the inverse of the CDF of <script type="math/tex">F(X)</script>.</li>
  <li>We find X by computing <script type="math/tex">F^{-1}(u)</script>.  <script type="math/tex">X</script> is now distributed according to <script type="math/tex">F(X)</script>.</li>
</ol>

<p>The Inverse Transform method warrants its own blog post so I won’t go into detail about its intrinsics and its edge cases and so forth.</p>

<p align="center" style="color:red;">
<b>
	Go back to the book and come back right after you’ve obtained the inverse of the CDF. 
</b>
</p>

<p>We have computed a Monte Carlo estimation of an integral, and now we’ve computed the inverse cdf of a PDF, which means we can transform our uniform random numbers into the PDF that we selected. If you recall, we said that we draw our samples from a uniform distribution from 1 to 0 in our original definition of the Monte Carlo estimator. The PDF of this estimator is <script type="math/tex">\frac{1}{1-0}</script> which is <script type="math/tex">1</script>. The more general Monte Carlo estimator definition is actually this:</p>

<script type="math/tex; mode=display">\frac{1}{N} \sum_{i=0}^{N-1} \frac{f(X_i)}{p(X_i)}</script>

<p>Where <script type="math/tex">p(X_i)</script> is the PDF from which the samples are drawn. By selecting a PDF that matches the shape of the function that we are trying to estimate, we are weighting the samples so as to reduce the variance. This is known as Importance Sampling! Choosing a good PDF is an ongoing form of research. For our integral, the PDF Dr Shirley’s picked is <script type="math/tex">\frac{1}{r}</script>.</p>

<p>For the moment, we’ve been drawing samples from the uniform distribution, but thanks to the inverse transform method, we’re now using these uniform numbers and transforming them to match the new PDF. We’ll then compute our original integral with these new random variables and divide by the new PDF.</p>

<p align="center" style="color:red;">
<b>
Now would be a good time to finish the chapter.
</b>
</p>

<p>If you feel more mathematically inclined, <a href="https://twitter.com/Atrix256" target="_blank">Alan Wolfe</a> has a great <a href="https://blog.demofox.org/2018/06/12/monte-carlo-integration-explanation-in-1d/" target="_blank">blog post about One-Dimensional Monte Carlo Integration</a>. He’s also got a plethora of posts that will have some common content with this blog post series. I would also suggest you have a look at <a href="http://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration.html" target="_blank">PBRT Chapter 13 </a> if you’re feeling brave!</p>

<p>If you remember, we spoke about stratification previously. Let’s take the original integral <script type="math/tex">\int_0^2 x^2</script>. To stratify the points, we’ll divide the interval in 4 bins. If I am taking N samples, we are going to take <script type="math/tex">\frac{n}{4}</script> samples per bin, so let’s call this value <script type="math/tex">k</script>. The Monte Carlo estimate using stratified sampling now looks like this:</p>

<script type="math/tex; mode=display">\int_0^2 x^2 = (0.5 - 0)\frac{1}{k} \sum_{i=0}^{k} x_1^2 + 
(1.0 - 0.5)\frac{1}{k} \sum_{i=0}^{k} x_2^2 +
\\
(1.5 - 1)\frac{1}{k} \sum_{i=0}^{k} x_3^2 +
(2 - 1.5)\frac{1}{k} \sum_{i=0}^{k} x_4^2</script>

<p><script type="math/tex">x_1</script> is a random number between 0 and 0.5, <script type="math/tex">x_2</script> is between 0.5 and 1.0, <script type="math/tex">x_3</script> between 1.0 and 1.5, and <script type="math/tex">x_4</script> between 1.5 and 2, giving us a total of <script type="math/tex">N</script> samples.</p>

<p>Here is a self contained program showing the stratified and naive Monte Carlo estimations.</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="cp">#include &lt;iostream&gt;
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
	<span class="kt">float</span> <span class="n">simpleMcRes</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">samples</span> <span class="o">=</span> <span class="mi">1000000</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="p">;</span>

	<span class="kt">int</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">samplesPerBin</span> <span class="o">=</span> <span class="n">samples</span> <span class="o">/</span> <span class="n">bins</span><span class="p">;</span>

	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">bins</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="kt">float</span> <span class="n">binStart</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="p">)</span> <span class="o">/</span> <span class="kt">float</span><span class="p">(</span><span class="n">bins</span><span class="p">);</span>

		<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">binStart</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
		<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">samplesPerBin</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
		<span class="p">{</span>
			<span class="kt">float</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">drand48</span><span class="p">();</span>
			<span class="n">simpleMcRes</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>

			<span class="n">x</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">drand48</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="p">)</span><span class="o">/</span><span class="kt">float</span><span class="p">(</span><span class="n">bins</span><span class="p">);</span>
			<span class="n">res</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span> 
		<span class="p">}</span>
	<span class="p">}</span>

	<span class="n">simpleMcRes</span> <span class="o">=</span> <span class="mi">2</span><span class="p">.</span><span class="mi">0</span> <span class="o">*</span> <span class="n">simpleMcRes</span> <span class="o">/</span> <span class="kt">float</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>
	<span class="n">res</span> <span class="o">=</span> <span class="mi">2</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span> <span class="o">*</span> <span class="n">res</span> <span class="o">/</span> <span class="kt">float</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>

	<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Simple MC: "</span> <span class="o">&lt;&lt;</span> <span class="n">simpleMcRes</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
	<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Stratified MC: "</span> <span class="o">&lt;&lt;</span> <span class="n">res</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<p>Stratification warrants its own post following some discussions with some friends. I wanted to show certain properties but felt that this post would be too long.</p>

<p>That’s pretty much it for this post, next up is Chapter 3 and 4.</p>

<p>If you have any questions and feedback, feel free to comment or contact me via twitter/email.</p>


    
    
    <hr><div id="disqus_thread"></div>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
  </article>
</main>


<footer>
    <a href="/rendering/2019/02/27/rt_rc_chapter3_4/">« Ray Tracing The Rest of Your Life: A reader's companion, Chapter 3 and 4</a>
    <a href="/rendering/2019/02/15/rt_rc_chapter1/">Ray Tracing The Rest of Your Life: A reader's companion, Chapter 1 »</a>
</footer>

